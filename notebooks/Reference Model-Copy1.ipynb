{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level 5 Kaggle Reference Model\n",
    "Author: **Guido Zuidhof** - [gzuidhof@lyft.com](mailto:gzuidhof@lyft.com)\n",
    "\n",
    "#### D. Inference and postprocessing\n",
    "4. Predicting our validation set.\n",
    "5. Thresholding the probability map.\n",
    "6. Performing a morphological closing operation to filter out tiny objects (presuming they are false positives)\n",
    "7. Loading the ground truth\n",
    "8. backprojecting our predicted boxes into world space\n",
    "\n",
    "#### E. Visualizing the results (not included in this kernel)\n",
    "x. Creating top down visualizations of the ground truth and predictions using the nuScenes SDK.  \n",
    "x. (Optional) Creating a GIF of a scene.  \n",
    "\n",
    "#### F. Evaluation\n",
    "x. Computing mAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T20:18:52.256983Z",
     "start_time": "2019-09-17T20:18:52.255270Z"
    },
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Our code will generate data, visualization and model checkpoints, they will be persisted to disk in this folder\n",
    "ARTIFACTS_FOLDER = \"./artifacts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T20:18:52.665636Z",
     "start_time": "2019-09-17T20:18:52.536338Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bev_train_data\t\t      unet_checkpoint_epoch_15.pth\r\n",
      "bev_validation_data\t      unet_checkpoint_epoch_1.pth\r\n",
      "detection_boxes.npy\t      unet_checkpoint_epoch_2.pth\r\n",
      "detection_classes.npy\t      unet_checkpoint_epoch_3.pth\r\n",
      "detection_scores.npy\t      unet_checkpoint_epoch_4.pth\r\n",
      "pred_box3ds.npy\t\t      unet_checkpoint_epoch_5.pth\r\n",
      "unet_checkpoint_epoch_10.pth  unet_checkpoint_epoch_6.pth\r\n",
      "unet_checkpoint_epoch_11.pth  unet_checkpoint_epoch_7.pth\r\n",
      "unet_checkpoint_epoch_12.pth  unet_checkpoint_epoch_8.pth\r\n",
      "unet_checkpoint_epoch_13.pth  unet_checkpoint_epoch_9.pth\r\n",
      "unet_checkpoint_epoch_14.pth\r\n"
     ]
    }
   ],
   "source": [
    "!ls {ARTIFACTS_FOLDER}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T20:18:53.862874Z",
     "start_time": "2019-09-17T20:18:52.964325Z"
    }
   },
   "outputs": [],
   "source": [
    "import pdb\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "import glob\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Disable multiprocesing for numpy/opencv. We already multiprocess ourselves, this would mean every subprocess produces\n",
    "# even more threads which would lead to a lot of context switching, slowing things down a lot.\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import scipy\n",
    "import scipy.ndimage\n",
    "import scipy.special\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "from lyft_dataset_sdk.lyftdataset import LyftDataset\n",
    "from lyft_dataset_sdk.utils.data_classes import LidarPointCloud, Box, Quaternion\n",
    "from lyft_dataset_sdk.utils.geometry_utils import view_points, transform_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T20:18:53.865596Z",
     "start_time": "2019-09-17T20:18:53.863861Z"
    }
   },
   "outputs": [],
   "source": [
    "base_dir = '../data/lyft/lyft_trainval/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T20:19:08.664139Z",
     "start_time": "2019-09-17T20:18:54.326367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 category,\n",
      "18 attribute,\n",
      "4 visibility,\n",
      "18421 instance,\n",
      "10 sensor,\n",
      "148 calibrated_sensor,\n",
      "177789 ego_pose,\n",
      "180 log,\n",
      "180 scene,\n",
      "22680 sample,\n",
      "189504 sample_data,\n",
      "638179 sample_annotation,\n",
      "1 map,\n",
      "Done loading in 12.0 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 2.3 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "level5data = LyftDataset(data_path=base_dir, json_path=f'{base_dir}/v1.0-trainval', verbose=True)\n",
    "os.makedirs(ARTIFACTS_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing a dataframe of scenes, with one scene per row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T20:19:08.667423Z",
     "start_time": "2019-09-17T20:19:08.665178Z"
    }
   },
   "outputs": [],
   "source": [
    "# \"bev\" stands for birds eye view\n",
    "classes = [\"car\", \"motorcycle\", \"bus\", \"bicycle\", \"truck\", \"pedestrian\", \"other_vehicle\", \"animal\", \"emergency_vehicle\"]\n",
    "train_data_folder = os.path.join(ARTIFACTS_FOLDER, \"bev_train_data\")\n",
    "validation_data_folder = os.path.join(ARTIFACTS_FOLDER, \"./bev_validation_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lidar not able to load: lidar tokens\n",
    "9cb04b1a4d476fd0782431764c7b55e91c6dbcbc6197c3dab3e044f13d058011"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Training a network to segment objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T20:19:09.053577Z",
     "start_time": "2019-09-17T20:19:08.668950Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "\n",
    "class BEVImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input_filepaths, target_filepaths, map_filepaths=None):\n",
    "        self.input_filepaths = input_filepaths\n",
    "        self.target_filepaths = target_filepaths\n",
    "        self.map_filepaths = map_filepaths\n",
    "        \n",
    "        if map_filepaths is not None:\n",
    "            assert len(input_filepaths) == len(map_filepaths)\n",
    "        \n",
    "        assert len(input_filepaths) == len(target_filepaths)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_filepaths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_filepath = self.input_filepaths[idx]\n",
    "        target_filepath = self.target_filepaths[idx]\n",
    "        \n",
    "        sample_token = input_filepath.split(\"/\")[-1].replace(\"_input.png\",\"\")\n",
    "        \n",
    "        im = cv2.imread(input_filepath, cv2.IMREAD_UNCHANGED)\n",
    "        \n",
    "        if self.map_filepaths:\n",
    "            map_filepath = self.map_filepaths[idx]\n",
    "            map_im = cv2.imread(map_filepath, cv2.IMREAD_UNCHANGED)\n",
    "            im = np.concatenate((im, map_im), axis=2)\n",
    "        \n",
    "        target = cv2.imread(target_filepath, cv2.IMREAD_UNCHANGED)\n",
    "        \n",
    "        im = im.astype(np.float32)/255\n",
    "        target = target.astype(np.int64)\n",
    "        \n",
    "        im = torch.from_numpy(im.transpose(2,0,1))\n",
    "        target = torch.from_numpy(target)\n",
    "        \n",
    "        return im, target, sample_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T20:19:09.067964Z",
     "start_time": "2019-09-17T20:19:09.054587Z"
    }
   },
   "outputs": [],
   "source": [
    "# This implementation was copied from https://github.com/jvanvugt/pytorch-unet, it is MIT licensed.\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=1,\n",
    "        n_classes=2,\n",
    "        depth=5,\n",
    "        wf=6,\n",
    "        padding=False,\n",
    "        batch_norm=False,\n",
    "        up_mode='upconv',\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Implementation of\n",
    "        U-Net: Convolutional Networks for Biomedical Image Segmentation\n",
    "        (Ronneberger et al., 2015)\n",
    "        https://arxiv.org/abs/1505.04597\n",
    "        Using the default arguments will yield the exact version used\n",
    "        in the original paper\n",
    "        Args:\n",
    "            in_channels (int): number of input channels\n",
    "            n_classes (int): number of output channels\n",
    "            depth (int): depth of the network\n",
    "            wf (int): number of filters in the first layer is 2**wf\n",
    "            padding (bool): if True, apply padding such that the input shape\n",
    "                            is the same as the output.\n",
    "                            This may introduce artifacts\n",
    "            batch_norm (bool): Use BatchNorm after layers with an\n",
    "                               activation function\n",
    "            up_mode (str): one of 'upconv' or 'upsample'.\n",
    "                           'upconv' will use transposed convolutions for\n",
    "                           learned upsampling.\n",
    "                           'upsample' will use bilinear upsampling.\n",
    "        \"\"\"\n",
    "        super(UNet, self).__init__()\n",
    "        assert up_mode in ('upconv', 'upsample')\n",
    "        self.padding = padding\n",
    "        self.depth = depth\n",
    "        prev_channels = in_channels\n",
    "        self.down_path = nn.ModuleList()\n",
    "        for i in range(depth):\n",
    "            self.down_path.append(\n",
    "                UNetConvBlock(prev_channels, 2 ** (wf + i), padding, batch_norm)\n",
    "            )\n",
    "            prev_channels = 2 ** (wf + i)\n",
    "\n",
    "        self.up_path = nn.ModuleList()\n",
    "        for i in reversed(range(depth - 1)):\n",
    "            self.up_path.append(\n",
    "                UNetUpBlock(prev_channels, 2 ** (wf + i), up_mode, padding, batch_norm)\n",
    "            )\n",
    "            prev_channels = 2 ** (wf + i)\n",
    "\n",
    "        self.last = nn.Conv2d(prev_channels, n_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        blocks = []\n",
    "        for i, down in enumerate(self.down_path):\n",
    "            x = down(x)\n",
    "            if i != len(self.down_path) - 1:\n",
    "                blocks.append(x)\n",
    "                x = F.max_pool2d(x, 2)\n",
    "\n",
    "        for i, up in enumerate(self.up_path):\n",
    "            x = up(x, blocks[-i - 1])\n",
    "\n",
    "        return self.last(x)\n",
    "\n",
    "\n",
    "class UNetConvBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size, padding, batch_norm):\n",
    "        super(UNetConvBlock, self).__init__()\n",
    "        block = []\n",
    "\n",
    "        block.append(nn.Conv2d(in_size, out_size, kernel_size=3, padding=int(padding)))\n",
    "        block.append(nn.ReLU())\n",
    "        if batch_norm:\n",
    "            block.append(nn.BatchNorm2d(out_size))\n",
    "\n",
    "        block.append(nn.Conv2d(out_size, out_size, kernel_size=3, padding=int(padding)))\n",
    "        block.append(nn.ReLU())\n",
    "        if batch_norm:\n",
    "            block.append(nn.BatchNorm2d(out_size))\n",
    "\n",
    "        self.block = nn.Sequential(*block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.block(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class UNetUpBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size, up_mode, padding, batch_norm):\n",
    "        super(UNetUpBlock, self).__init__()\n",
    "        if up_mode == 'upconv':\n",
    "            self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=2, stride=2)\n",
    "        elif up_mode == 'upsample':\n",
    "            self.up = nn.Sequential(\n",
    "                nn.Upsample(mode='bilinear', scale_factor=2),\n",
    "                nn.Conv2d(in_size, out_size, kernel_size=1),\n",
    "            )\n",
    "\n",
    "        self.conv_block = UNetConvBlock(in_size, out_size, padding, batch_norm)\n",
    "\n",
    "    def center_crop(self, layer, target_size):\n",
    "        _, _, layer_height, layer_width = layer.size()\n",
    "        diff_y = (layer_height - target_size[0]) // 2\n",
    "        diff_x = (layer_width - target_size[1]) // 2\n",
    "        return layer[\n",
    "            :, :, diff_y : (diff_y + target_size[0]), diff_x : (diff_x + target_size[1])\n",
    "        ]\n",
    "\n",
    "    def forward(self, x, bridge):\n",
    "        up = self.up(x)\n",
    "        crop1 = self.center_crop(bridge, up.shape[2:])\n",
    "        out = torch.cat([up, crop1], 1)\n",
    "        out = self.conv_block(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train a U-net fully convolutional neural network, we create a network that is less deep and with only half the amount of filters compared to the original U-net paper implementation. We do this to keep training and inference time low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T20:19:09.079478Z",
     "start_time": "2019-09-17T20:19:09.069000Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_unet_model(in_channels=3, num_output_classes=2):\n",
    "    model = UNet(in_channels=in_channels, n_classes=num_output_classes, wf=5, depth=4, padding=True, up_mode='upsample')\n",
    "    # Optional, for multi GPU training and inference\n",
    "    model = nn.DataParallel(model)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T20:19:12.743214Z",
     "start_time": "2019-09-17T20:19:12.736719Z"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_predictions(input_image, prediction, target, n_images=2, apply_softmax=True):\n",
    "    \"\"\"\n",
    "    Takes as input 3 PyTorch tensors, plots the input image, predictions and targets.\n",
    "    \"\"\"\n",
    "    # Only select the first n images\n",
    "    prediction = prediction[:n_images]\n",
    "    target = target[:n_images]\n",
    "    input_image = input_image[:n_images]\n",
    "\n",
    "    prediction = prediction.detach().cpu().numpy()\n",
    "    if apply_softmax:\n",
    "        prediction = scipy.special.softmax(prediction, axis=1)\n",
    "    class_one_preds = np.hstack(1-prediction[:,0])\n",
    "\n",
    "    target = np.hstack(target.detach().cpu().numpy())\n",
    "\n",
    "    class_rgb = np.repeat(class_one_preds[..., None], 3, axis=2)\n",
    "    class_rgb[...,2] = 0\n",
    "    class_rgb[...,1] = target\n",
    "\n",
    "    \n",
    "    input_im = np.hstack(input_image.cpu().numpy().transpose(0,2,3,1))\n",
    "    \n",
    "    if input_im.shape[2] == 3:\n",
    "        input_im_grayscale = np.repeat(input_im.mean(axis=2)[..., None], 3, axis=2)\n",
    "        overlayed_im = (input_im_grayscale*0.6 + class_rgb*0.7).clip(0,1)\n",
    "    else:\n",
    "        input_map = input_im[...,3:]\n",
    "        overlayed_im = (input_map*0.6 + class_rgb*0.7).clip(0,1)\n",
    "\n",
    "    thresholded_pred = np.repeat(class_one_preds[..., None] > 0.5, 3, axis=2)\n",
    "\n",
    "    fig = plt.figure(figsize=(12,26))\n",
    "    plot_im = np.vstack([class_rgb, input_im[...,:3], overlayed_im, thresholded_pred]).clip(0,1).astype(np.float32)\n",
    "    plt.imshow(plot_im)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T20:19:16.509674Z",
     "start_time": "2019-09-17T20:19:13.559269Z"
    }
   },
   "outputs": [],
   "source": [
    "# We weigh the loss for the 0 class lower to account for (some of) the big class imbalance.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "class_weights = torch.from_numpy(np.array([0.2] + [1.0]*len(classes), dtype=np.float32))\n",
    "class_weights = class_weights.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T20:19:16.538856Z",
     "start_time": "2019-09-17T20:19:16.510697Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.2000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000], device='cuda:0'),\n",
       " ['car',\n",
       "  'motorcycle',\n",
       "  'bus',\n",
       "  'bicycle',\n",
       "  'truck',\n",
       "  'pedestrian',\n",
       "  'other_vehicle',\n",
       "  'animal',\n",
       "  'emergency_vehicle'],\n",
       " torch.Size([10]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights, classes, class_weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T19:13:30.265203Z",
     "start_time": "2019-09-17T19:13:30.248423Z"
    }
   },
   "source": [
    "total 10 classes: background + 9 categories, 0.2 weight to bg, rest 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T13:49:34.556915Z",
     "start_time": "2019-09-17T12:09:37.983453Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "epochs = 15 # Note: We may be able to train for longer and expect better results, the reason this number is low is to keep the runtime short.\n",
    "\n",
    "model = get_unet_model(num_output_classes=len(classes)+1)\n",
    "model = model.to(device)\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "dataloader = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle=True, num_workers=os.cpu_count()*2)\n",
    "\n",
    "all_losses = []\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    print(\"Epoch\", epoch)\n",
    "    \n",
    "    epoch_losses = []\n",
    "    progress_bar = tqdm_notebook(dataloader)\n",
    "    \n",
    "    for ii, (X, target, sample_ids) in enumerate(progress_bar):\n",
    "        X = X.to(device)  # [N, 3, H, W]\n",
    "        target = target.to(device)  # [N, H, W] with class indices (0, 1)\n",
    "        prediction = model(X)  # [N, 2, H, W]\n",
    "        loss = F.cross_entropy(prediction, target, weight=class_weights)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        epoch_losses.append(loss.detach().cpu().numpy())\n",
    "\n",
    "        if ii == 0:\n",
    "            visualize_predictions(X, prediction, target)\n",
    "    \n",
    "    print(\"Loss:\", np.mean(epoch_losses))\n",
    "    all_losses.extend(epoch_losses)\n",
    "    \n",
    "    checkpoint_filename = \"unet_checkpoint_epoch_{}.pth\".format(epoch)\n",
    "    checkpoint_filepath = os.path.join(ARTIFACTS_FOLDER, checkpoint_filename)\n",
    "    torch.save(model.state_dict(), checkpoint_filepath)\n",
    "    \n",
    "plt.figure(figsize=(12,12))\n",
    "plt.plot(all_losses, alpha=0.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can interpret the above visualizations as follows:  \n",
    "There are four different visualizations stacked on top of eachother:\n",
    "1. The top images have two color channels: red for predictions, green for targets. Note that red+green=yellow. In other words:  \n",
    "> **Black**: True Negative  \n",
    "**Green**: False Negative  \n",
    "**Yellow**: True Positive  \n",
    "**Red**: False Positive \n",
    "2. The input image\n",
    "3. The input image (or semantic input map, not in this kernel) blended together with targets+predictions\n",
    "4. The predictions thresholded at 0.5 probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### val set predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T20:19:30.954905Z",
     "start_time": "2019-09-17T20:19:30.270344Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_filepaths = sorted(glob.glob(os.path.join(validation_data_folder, \"*_input.png\")))\n",
    "target_filepaths = sorted(glob.glob(os.path.join(validation_data_folder, \"*_target.png\")))\n",
    "\n",
    "batch_size=16\n",
    "validation_dataset = BEVImageDataset(input_filepaths, target_filepaths)\n",
    "validation_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size, shuffle=False, num_workers=os.cpu_count())\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = get_unet_model(num_output_classes=1+len(classes))\n",
    "model = model.to(device)\n",
    "\n",
    "epoch_to_load=15\n",
    "checkpoint_filename = \"unet_checkpoint_epoch_{}.pth\".format(epoch_to_load)\n",
    "checkpoint_filepath = os.path.join(ARTIFACTS_FOLDER, checkpoint_filename)\n",
    "model.load_state_dict(torch.load(checkpoint_filepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T20:20:28.946945Z",
     "start_time": "2019-09-17T20:20:15.971087Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef4759251c374faea997ea2c6ead5080",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=315), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ags/miniconda3/envs/ML/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/ags/miniconda3/envs/ML/lib/python3.7/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "progress_bar = tqdm_notebook(validation_dataloader)\n",
    "\n",
    "targets = np.zeros((len(target_filepaths), 336, 336), dtype=np.uint8)\n",
    "\n",
    "# We quantize to uint8 here to conserve memory. We're allocating >20GB of memory otherwise.\n",
    "predictions = np.zeros((len(target_filepaths), 1+len(classes), 336, 336), dtype=np.uint8)\n",
    "\n",
    "sample_tokens = []\n",
    "all_losses = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for ii, (X, target, batch_sample_tokens) in enumerate(progress_bar):\n",
    "\n",
    "        offset = ii*batch_size\n",
    "        targets[offset:offset+batch_size] = target.numpy()\n",
    "        sample_tokens.extend(batch_sample_tokens)\n",
    "#         continue\n",
    "        X = X.to(device)  # [N, 1, H, W]\n",
    "        target = target.to(device)  # [N, H, W] with class indices (0, 1)\n",
    "        prediction = model(X)  # [N, 2, H, W]\n",
    "        loss = F.cross_entropy(prediction, target, weight=class_weights)\n",
    "        all_losses.append(loss.detach().cpu().numpy())\n",
    "        \n",
    "        prediction = F.softmax(prediction, dim=1)\n",
    "        \n",
    "        prediction_cpu = prediction.cpu().numpy()\n",
    "        predictions[offset:offset+batch_size] = np.round(prediction_cpu*255).astype(np.uint8)\n",
    "        \n",
    "        # Visualize the first prediction\n",
    "        if ii == 0:\n",
    "            pass\n",
    "#             visualize_predictions(X, prediction, target, apply_softmax=False)\n",
    "            \n",
    "print(\"Mean loss:\", np.mean(all_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T20:02:44.352791Z",
     "start_time": "2019-09-17T20:02:44.301327Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([253.,   2.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(prediction_cpu * 255)[0][:, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T20:02:45.775929Z",
     "start_time": "2019-09-17T20:02:45.773113Z"
    }
   },
   "outputs": [],
   "source": [
    "# del model, class_weights, validation_dataloader, validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T20:02:46.274707Z",
     "start_time": "2019-09-17T20:02:46.270411Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5040, 10, 336, 336), torch.Size([16, 10, 336, 336]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape, prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T20:02:47.406248Z",
     "start_time": "2019-09-17T20:02:47.401516Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([252,   2,   0,   0,   0,   0,   0,   0,   0,   0], dtype=uint8), 9)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0][:, 0, 0], len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T20:07:26.136898Z",
     "start_time": "2019-09-17T20:07:25.944484Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get probabilities for non-background\n",
    "predictions_non_class0 = 255 - predictions[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T20:02:54.358660Z",
     "start_time": "2019-09-17T20:02:54.355908Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5040, 336, 336)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_non_class0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T20:03:09.125866Z",
     "start_time": "2019-09-17T20:03:09.119317Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor i in range(3):\\n    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(16, 6))\\n    axes[0].imshow(predictions_non_class0[i])\\n    axes[0].set_title(\"predictions\")\\n    axes[1].imshow(predictions_non_class0[i] > background_threshold)\\n    axes[1].set_title(\"thresholded predictions\")\\n    axes[2].imshow((targets[i] > 0).astype(np.uint8), interpolation=\"nearest\")\\n    axes[2].set_title(\"targets\")\\n    fig.tight_layout()\\n    fig.show()\\n'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Arbitrary threshold in our system to create a binary image to fit boxes around.\n",
    "background_threshold = 255//2\n",
    "\n",
    "#for i in range(3):\n",
    "#    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(16, 6))\n",
    "#    axes[0].imshow(predictions_non_class0[i])\n",
    "#    axes[0].set_title(\"predictions\")\n",
    "#    axes[1].imshow(predictions_non_class0[i] > background_threshold)\n",
    "#    axes[1].set_title(\"thresholded predictions\")\n",
    "#    axes[2].imshow((targets[i] > 0).astype(np.uint8), interpolation=\"nearest\")\n",
    "#    axes[2].set_title(\"targets\")\n",
    "#    fig.tight_layout()\n",
    "#    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T20:03:33.114513Z",
     "start_time": "2019-09-17T20:03:32.641587Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5040/5040 [00:00<00:00, 10966.43it/s]\n"
     ]
    }
   ],
   "source": [
    "# We perform an opening morphological operation to filter tiny detections\n",
    "# Note that this may be problematic for classes that are inherently small (e.g. pedestrians)..\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(3,3))\n",
    "predictions_opened = np.zeros((predictions_non_class0.shape), dtype=np.uint8)\n",
    "\n",
    "for i, p in enumerate(tqdm(predictions_non_class0)):\n",
    "    thresholded_p = (p > background_threshold).astype(np.uint8)\n",
    "    predictions_opened[i] = cv2.morphologyEx(thresholded_p, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "#plt.figure(figsize=(12,12))\n",
    "#fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 6))\n",
    "#axes[0].imshow(predictions_non_class0[0] > 255//2)\n",
    "#axes[0].set_title(\"thresholded prediction\")\n",
    "#axes[1].imshow(predictions_opened[0])\n",
    "#axes[1].set_title(\"opened thresholded prediction\")\n",
    "#fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T20:03:50.568468Z",
     "start_time": "2019-09-17T20:03:50.563028Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sanity check: let's count the amount of connected components in an image\n",
    "# labels, n = scipy.ndimage.label(predictions_opened[0])\n",
    "# plt.imshow(labels, cmap=\"tab20b\")\n",
    "# plt.xlabel(\"N predictions: {}\".format(n))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above image looks pretty well separated, some boxes seem to be wrongly merged together and may be problematic. Let's continue.\n",
    "For each scene we fit boxes to the segmentations. For each box and each class we write it's probability in the center pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T19:21:27.986066Z",
     "start_time": "2019-09-17T19:21:27.981287Z"
    }
   },
   "outputs": [],
   "source": [
    "# del labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T20:09:48.253337Z",
     "start_time": "2019-09-17T20:09:48.245765Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1], dtype=uint8),\n",
       " array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "         13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "         26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "         39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "         52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "         65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "         91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "        104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "        117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "        130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "        143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
       "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "        169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
       "        195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
       "        208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
       "        221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
       "        234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
       "        247, 248, 249, 250, 251, 252, 253, 254, 255], dtype=uint8))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(predictions_opened[0]), np.unique(predictions_non_class0[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T20:08:01.370831Z",
     "start_time": "2019-09-17T20:08:01.363485Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5040, 336, 336), (5040, 336, 336), (5040, 10, 336, 336))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_opened.shape, predictions_non_class0.shape, predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T20:13:33.509486Z",
     "start_time": "2019-09-17T20:13:29.291973Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "783b4436d06348c0af9024bd2ac421f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5040), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total amount of boxes: 148861\n"
     ]
    }
   ],
   "source": [
    "detection_boxes = []\n",
    "detection_scores = []\n",
    "detection_classes = []\n",
    "\n",
    "for i in tqdm_notebook(range(len(predictions))):\n",
    "    prediction_opened = predictions_opened[i]\n",
    "    probability_non_class0 = predictions_non_class0[i]\n",
    "    class_probability = predictions[i]\n",
    "\n",
    "    sample_boxes = []\n",
    "    sample_detection_scores = []\n",
    "    sample_detection_classes = []\n",
    "    \n",
    "    contours, hierarchy = cv2.findContours(prediction_opened, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE) \n",
    "    \n",
    "    for cnt in contours:\n",
    "        rect = cv2.minAreaRect(cnt)\n",
    "        box = cv2.boxPoints(rect)\n",
    "        \n",
    "        # Let's take the center pixel value as the confidence value\n",
    "        box_center_index = np.int0(np.mean(box, axis=0))\n",
    "        \n",
    "        for class_index in range(len(classes)):\n",
    "            box_center_value = class_probability[class_index+1, box_center_index[1], box_center_index[0]] # notice the [1, 0] ?\n",
    "            \n",
    "            # Let's remove candidates with very low probability\n",
    "            if box_center_value < 0.01:\n",
    "                continue\n",
    "            \n",
    "            box_center_class = classes[class_index]\n",
    "\n",
    "            box_detection_score = box_center_value\n",
    "            sample_detection_classes.append(box_center_class)\n",
    "            sample_detection_scores.append(box_detection_score)\n",
    "            sample_boxes.append(box)\n",
    "        \n",
    "    \n",
    "    detection_boxes.append(np.array(sample_boxes))\n",
    "    detection_scores.append(sample_detection_scores)\n",
    "    detection_classes.append(sample_detection_classes)\n",
    "    \n",
    "print(\"Total amount of boxes:\", np.sum([len(x) for x in detection_boxes]))\n",
    "    \n",
    "# # Visualize the boxes in the first sample\n",
    "# t = np.zeros_like(predictions_opened[0])\n",
    "# for sample_boxes in detection_boxes[0]:\n",
    "#     box_pix = np.int0(sample_boxes)\n",
    "#     cv2.drawContours(t,[box_pix],0,(255),2)\n",
    "# plt.imshow(t)\n",
    "# plt.show()\n",
    "\n",
    "# # Visualize their probabilities\n",
    "# plt.hist(detection_scores[0], bins=20)\n",
    "# plt.xlabel(\"Detection Score\")\n",
    "# plt.ylabel(\"Count\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the ground truth for the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T14:47:38.662610Z",
     "start_time": "2019-09-17T14:47:38.289160Z"
    }
   },
   "outputs": [],
   "source": [
    "# !export DYLD_FALLBACK_LIBRARY_PATH=$(HOME)/lib:/usr/local/lib:/lib:/usr/lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T20:21:20.174965Z",
     "start_time": "2019-09-17T20:20:57.993144Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e134fa8a2eaa4dc0a4492db8d91a53f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5040), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from lyft_dataset_sdk.eval.detection.mAP_evaluation import Box3D, recall_precision\n",
    "\n",
    "def load_groundtruth_boxes(nuscenes, sample_tokens):\n",
    "    gt_box3ds = []\n",
    "\n",
    "    # Load annotations and filter predictions and annotations.\n",
    "    for sample_token in tqdm_notebook(sample_tokens):\n",
    "\n",
    "        sample = nuscenes.get('sample', sample_token)\n",
    "        sample_annotation_tokens = sample['anns']\n",
    "\n",
    "        sample_lidar_token = sample[\"data\"][\"LIDAR_TOP\"]\n",
    "        lidar_data = level5data.get(\"sample_data\", sample_lidar_token)\n",
    "        ego_pose = level5data.get(\"ego_pose\", lidar_data[\"ego_pose_token\"])\n",
    "        ego_translation = np.array(ego_pose['translation'])\n",
    "        \n",
    "        for sample_annotation_token in sample_annotation_tokens:\n",
    "            sample_annotation = nuscenes.get('sample_annotation', sample_annotation_token)\n",
    "            sample_annotation_translation = sample_annotation['translation']\n",
    "            \n",
    "            class_name = sample_annotation['category_name']\n",
    "            \n",
    "            box3d = Box3D(\n",
    "                sample_token=sample_token,\n",
    "                translation=sample_annotation_translation,\n",
    "                size=sample_annotation['size'],\n",
    "                rotation=sample_annotation['rotation'],\n",
    "                name=class_name\n",
    "            )\n",
    "            gt_box3ds.append(box3d)\n",
    "            \n",
    "    return gt_box3ds\n",
    "\n",
    "gt_box3ds = load_groundtruth_boxes(level5data, sample_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we take our predicted boxes, transform them back into world space and make them 3D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T20:23:10.365553Z",
     "start_time": "2019-09-17T20:23:06.136014Z"
    }
   },
   "outputs": [],
   "source": [
    "# np.save(os.path.join(ARTIFACTS_FOLDER, 'gt_box3ds.npy'), gt_box3ds)\n",
    "# np.save(os.path.join(ARTIFACTS_FOLDER, 'detection_boxes.npy'), detection_boxes)\n",
    "# np.save(os.path.join(ARTIFACTS_FOLDER, 'detection_scores.npy'), detection_scores)\n",
    "# np.save(os.path.join(ARTIFACTS_FOLDER, 'detection_classes.npy'), detection_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T20:23:14.763857Z",
     "start_time": "2019-09-17T20:23:14.754341Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_transformation_matrix_to_voxel_space(shape, voxel_size, offset):\n",
    "    \"\"\"\n",
    "    Constructs a transformation matrix given an output voxel shape such that (0,0,0) ends up in the center.\n",
    "    Voxel_size defines how large every voxel is in world coordinate, (1,1,1) would be the same as Minecraft voxels.\n",
    "    \n",
    "    An offset per axis in world coordinates (metric) can be provided, this is useful for Z (up-down) in lidar points.\n",
    "    \"\"\"\n",
    "    \n",
    "    shape, voxel_size, offset = np.array(shape), np.array(voxel_size), np.array(offset)\n",
    "    \n",
    "    tm = np.eye(4, dtype=np.float32)\n",
    "    translation = shape/2 + offset/voxel_size\n",
    "    \n",
    "    tm = tm * np.array(np.hstack((1/voxel_size, [1])))\n",
    "    tm[:3, 3] = np.transpose(translation)\n",
    "    return tm\n",
    "\n",
    "def transform_points(points, transf_matrix):\n",
    "    \"\"\"\n",
    "    Transform (3,N) or (4,N) points using transformation matrix.\n",
    "    \"\"\"\n",
    "    if points.shape[0] not in [3,4]:\n",
    "        raise Exception(\"Points input should be (3,N) or (4,N) shape, received {}\".format(points.shape))\n",
    "    return transf_matrix.dot(np.vstack((points[:3, :], np.ones(points.shape[1]))))[:3, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T20:23:16.499274Z",
     "start_time": "2019-09-17T20:23:16.493756Z"
    }
   },
   "outputs": [],
   "source": [
    "# Some hyperparameters we'll need to define for the system\n",
    "voxel_size = (0.4, 0.4, 1.5)\n",
    "z_offset = -2.0\n",
    "bev_shape = (336, 336, 3)\n",
    "\n",
    "# We scale down each box so they are more separated when projected into our coarse voxel space.\n",
    "box_scale = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T20:23:18.492521Z",
     "start_time": "2019-09-17T20:23:18.372662Z"
    }
   },
   "outputs": [],
   "source": [
    "gt_box3ds = np.load(os.path.join(ARTIFACTS_FOLDER, 'gt_box3ds.npy'))\n",
    "detection_boxes = np.load(os.path.join(ARTIFACTS_FOLDER, 'detection_boxes.npy'))\n",
    "detection_scores = np.load(os.path.join(ARTIFACTS_FOLDER, 'detection_scores.npy'))\n",
    "detection_classes = np.load(os.path.join(ARTIFACTS_FOLDER, 'detection_classes.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T20:25:01.548367Z",
     "start_time": "2019-09-17T20:25:01.545333Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(143414, 5040, 5040, 5040)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gt_box3ds), len(detection_boxes), len(detection_scores), len(detection_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T20:25:28.909608Z",
     "start_time": "2019-09-17T20:25:28.901192Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[204.00003, 266.00003],\n",
       "       [197.50003, 259.50003],\n",
       "       [202.00003, 255.00003],\n",
       "       [208.50003, 261.50003]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detection_boxes[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T20:26:36.291209Z",
     "start_time": "2019-09-17T20:25:52.686471Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "774ea92b86454d968a3edf29ddf85af6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5040), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pred_box3ds = []\n",
    "\n",
    "# This could use some refactoring..\n",
    "for (sample_token, sample_boxes, sample_detection_scores, sample_detection_class) in tqdm_notebook(zip(sample_tokens, detection_boxes, detection_scores, detection_classes), total=len(sample_tokens)):\n",
    "    sample_boxes = sample_boxes.reshape(-1, 2) # (N, 4, 2) -> (N*4, 2)\n",
    "    sample_boxes = sample_boxes.transpose(1,0) # (N*4, 2) -> (2, N*4)\n",
    "\n",
    "    # Add Z dimension\n",
    "    sample_boxes = np.vstack((sample_boxes, np.zeros(sample_boxes.shape[1]),)) # (2, N*4) -> (3, N*4)\n",
    "\n",
    "    sample = level5data.get(\"sample\", sample_token)\n",
    "    sample_lidar_token = sample[\"data\"][\"LIDAR_TOP\"]\n",
    "    lidar_data = level5data.get(\"sample_data\", sample_lidar_token)\n",
    "    lidar_filepath = level5data.get_sample_data_path(sample_lidar_token)\n",
    "    ego_pose = level5data.get(\"ego_pose\", lidar_data[\"ego_pose_token\"])\n",
    "    ego_translation = np.array(ego_pose['translation'])\n",
    "\n",
    "    global_from_car = transform_matrix(ego_pose['translation'],\n",
    "                                       Quaternion(ego_pose['rotation']), inverse=False)\n",
    "\n",
    "    car_from_voxel = np.linalg.inv(create_transformation_matrix_to_voxel_space(bev_shape, voxel_size, (0, 0, z_offset)))\n",
    "\n",
    "\n",
    "    global_from_voxel = np.dot(global_from_car, car_from_voxel)\n",
    "    sample_boxes = transform_points(sample_boxes, global_from_voxel)\n",
    "\n",
    "    # We don't know at where the boxes are in the scene on the z-axis (up-down), let's assume all of them are at\n",
    "    # the same height as the ego vehicle.\n",
    "    sample_boxes[2,:] = ego_pose[\"translation\"][2]\n",
    "\n",
    "\n",
    "    # (3, N*4) -> (N, 4, 3)\n",
    "    sample_boxes = sample_boxes.transpose(1,0).reshape(-1, 4, 3)\n",
    "\n",
    "\n",
    "    # We don't know the height of our boxes, let's assume every object is the same height.\n",
    "    box_height = 1.75\n",
    "\n",
    "    # Note: Each of these boxes describes the ground corners of a 3D box.\n",
    "    # To get the center of the box in 3D, we'll have to add half the height to it.\n",
    "    sample_boxes_centers = sample_boxes.mean(axis=1)\n",
    "    sample_boxes_centers[:,2] += box_height/2\n",
    "\n",
    "    # Width and height is arbitrary - we don't know what way the vehicles are pointing from our prediction segmentation\n",
    "    # It doesn't matter for evaluation, so no need to worry about that here.\n",
    "    # Note: We scaled our targets to be 0.8 the actual size, we need to adjust for that\n",
    "    sample_lengths = np.linalg.norm(sample_boxes[:,0,:] - sample_boxes[:,1,:], axis=1) * 1/box_scale\n",
    "    sample_widths = np.linalg.norm(sample_boxes[:,1,:] - sample_boxes[:,2,:], axis=1) * 1/box_scale\n",
    "    \n",
    "    sample_boxes_dimensions = np.zeros_like(sample_boxes_centers) \n",
    "    sample_boxes_dimensions[:,0] = sample_widths\n",
    "    sample_boxes_dimensions[:,1] = sample_lengths\n",
    "    sample_boxes_dimensions[:,2] = box_height\n",
    "\n",
    "    for i in range(len(sample_boxes)):\n",
    "        translation = sample_boxes_centers[i]\n",
    "        size = sample_boxes_dimensions[i]\n",
    "        class_name = sample_detection_class[i]\n",
    "        ego_distance = float(np.linalg.norm(ego_translation - translation))\n",
    "    \n",
    "        \n",
    "        # Determine the rotation of the box\n",
    "        v = (sample_boxes[i,0] - sample_boxes[i,1])\n",
    "        v /= np.linalg.norm(v)\n",
    "        r = R.from_dcm([\n",
    "            [v[0], -v[1], 0],\n",
    "            [v[1],  v[0], 0],\n",
    "            [   0,     0, 1],\n",
    "        ])\n",
    "        quat = r.as_quat()\n",
    "        # XYZW -> WXYZ order of elements\n",
    "        quat = quat[[3,0,1,2]]\n",
    "        \n",
    "        detection_score = float(sample_detection_scores[i])\n",
    "\n",
    "        \n",
    "        box3d = Box3D(\n",
    "            sample_token=sample_token,\n",
    "            translation=list(translation),\n",
    "            size=list(size),\n",
    "            rotation=list(quat),\n",
    "            name=class_name,\n",
    "            score=detection_score\n",
    "        )\n",
    "        pred_box3ds.append(box3d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T20:26:51.233742Z",
     "start_time": "2019-09-17T20:26:40.591983Z"
    }
   },
   "outputs": [],
   "source": [
    "# np.save(os.path.join(ARTIFACTS_FOLDER, 'pred_box3ds.npy'), pred_box3ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T16:19:52.573596Z",
     "start_time": "2019-09-17T16:19:47.804154Z"
    }
   },
   "outputs": [],
   "source": [
    "gt_box3ds = np.load(os.path.join(ARTIFACTS_FOLDER, 'gt_box3ds.npy'))\n",
    "detection_boxes = np.load(os.path.join(ARTIFACTS_FOLDER, 'detection_boxes.npy'))\n",
    "detection_scores = np.load(os.path.join(ARTIFACTS_FOLDER, 'detection_scores.npy'))\n",
    "detection_classes = np.load(os.path.join(ARTIFACTS_FOLDER, 'detection_classes.npy'))\n",
    "pred_box3ds = np.load(os.path.join(ARTIFACTS_FOLDER, 'pred_box3ds.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T20:27:32.355765Z",
     "start_time": "2019-09-17T20:27:32.352784Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(148861, 143414)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred_box3ds), len(gt_box3ds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T20:27:38.567917Z",
     "start_time": "2019-09-17T20:27:38.564463Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sample_token': '00336df4f44fa605a6ef6a08e8b312564d13f385228abe9719af6daa543033b2', 'translation': [2612.483725582335, 768.8829575895404, -18.772330722393747], 'size': [3.1818760303387217, 4.596178485914876, 1.75], 'rotation': [0.9917494534411332, 0.0, 0.0, 0.1281913475988669], 'name': 'car', 'volume': 25.59282277210936, 'score': 211.0}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_box3ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T20:27:46.146678Z",
     "start_time": "2019-09-17T20:27:46.144149Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sample_token': '00336df4f44fa605a6ef6a08e8b312564d13f385228abe9719af6daa543033b2', 'translation': [2588.0412154737305, 744.1343975981417, -18.810591968737043], 'size': [1.653, 4.332, 1.444], 'rotation': [-0.9628021828085169, 0, 0, 0.27020724782869027], 'name': 'car', 'volume': 10.340189423999998, 'score': -1}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_box3ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_box3ds = [box.serialize() for box in gt_box3ds]\n",
    "pred_box3ds = [box.serialize() for box in pred_box3ds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T20:40:58.897312Z",
     "start_time": "2019-09-17T20:38:13.005554Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/ags/DATA/CODE/kaggle/lyft-3d-object-detection/notebooks/lyft_dataset_sdk/eval/detection/mAP_evaluation.py:312: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recalls = tp / float(num_gts)\n",
      "/media/ags/DATA/CODE/kaggle/lyft-3d-object-detection/notebooks/lyft_dataset_sdk/eval/detection/mAP_evaluation.py:314: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  assert np.all(0 <= recalls) & np.all(recalls <= 1)\n",
      "/media/ags/DATA/CODE/kaggle/lyft-3d-object-detection/notebooks/lyft_dataset_sdk/eval/detection/mAP_evaluation.py:314: RuntimeWarning: invalid value encountered in less_equal\n",
      "  assert np.all(0 <= recalls) & np.all(recalls <= 1)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-4f5d94a960ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlyft_dataset_sdk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmAP_evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0miou_threshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0maverage_precisions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmAP_evaluation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_average_precisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_box3ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_box3ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miou_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/media/ags/DATA/CODE/kaggle/lyft-3d-object-detection/notebooks/lyft_dataset_sdk/eval/detection/mAP_evaluation.py\u001b[0m in \u001b[0;36mget_average_precisions\u001b[0;34m(gt, predictions, class_names, iou_threshold)\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclass_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpred_by_class_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m             recalls, precisions, average_precision = recall_precision(\n\u001b[0;32m--> 369\u001b[0;31m                 \u001b[0mgt_by_class_name\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclass_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_by_class_name\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclass_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miou_threshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m             )\n\u001b[1;32m    371\u001b[0m             \u001b[0maverage_precisions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclass_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maverage_precision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/ags/DATA/CODE/kaggle/lyft-3d-object-detection/notebooks/lyft_dataset_sdk/eval/detection/mAP_evaluation.py\u001b[0m in \u001b[0;36mrecall_precision\u001b[0;34m(gt, predictions, iou_threshold)\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0mrecalls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_gts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrecalls\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecalls\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[0;31m# avoid divide by zero in case the first detection matches a difficult ground truth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from lyft_dataset_sdk.eval.detection import mAP_evaluation\n",
    "iou_threshold = 0.5\n",
    "average_precisions = mAP_evaluation.get_average_precisions(gt_box3ds, pred_box3ds, classes, iou_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T21:16:38.298115Z",
     "start_time": "2019-09-17T21:16:38.271004Z"
    }
   },
   "outputs": [],
   "source": [
    "gt_by_class_name = mAP_evaluation.group_by_key(gt_box3ds, \"name\")\n",
    "pred_by_class_name = mAP_evaluation.group_by_key(pred_box3ds, \"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T21:16:38.827362Z",
     "start_time": "2019-09-17T21:16:38.816925Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car 118839\n",
      "bicycle 5503\n",
      "other_vehicle 7009\n",
      "bus 2044\n",
      "truck 4002\n",
      "motorcycle 159\n",
      "pedestrian 5824\n",
      "animal 34\n"
     ]
    }
   ],
   "source": [
    "for key in gt_by_class_name.keys():\n",
    "    print(key, len(gt_by_class_name[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T21:16:39.944639Z",
     "start_time": "2019-09-17T21:16:39.941799Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['car', 'bicycle', 'other_vehicle', 'bus', 'truck', 'motorcycle', 'pedestrian', 'animal'])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_by_class_name.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T21:16:41.963529Z",
     "start_time": "2019-09-17T21:16:41.960353Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car 107244\n",
      "bicycle 8255\n",
      "pedestrian 8058\n",
      "bus 6178\n",
      "truck 5945\n",
      "other_vehicle 10603\n",
      "motorcycle 2006\n",
      "emergency_vehicle 543\n",
      "animal 29\n"
     ]
    }
   ],
   "source": [
    "for key in pred_by_class_name.keys():\n",
    "    print(key, len(pred_by_class_name[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T21:19:41.971803Z",
     "start_time": "2019-09-17T21:16:48.876301Z"
    }
   },
   "outputs": [],
   "source": [
    "average_precisions = np.zeros(len(classes))\n",
    "\n",
    "for class_id, class_name in enumerate(classes):\n",
    "    if class_name in pred_by_class_name and class_name in gt_by_class_name:\n",
    "        recalls, precisions, average_precision = recall_precision(\n",
    "            gt_by_class_name[class_name], pred_by_class_name[class_name], iou_threshold\n",
    "        )\n",
    "        average_precisions[class_id] = average_precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T21:35:40.917801Z",
     "start_time": "2019-09-17T21:35:40.912483Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.63153498e-01, 0.00000000e+00, 2.33167296e-04, 5.57413055e-05,\n",
       "       3.26573451e-05, 8.79200888e-05, 5.69355671e-04, 0.00000000e+00,\n",
       "       0.00000000e+00])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_precisions#.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ⚠️ Note: This kernel is a work in progress\n",
    "\n",
    "I didn't want to hold off on releasing this kernel as I think it will help with getting started in this competition as it is :). \n",
    "\n",
    "At this point we have `pred_box3ds` and `gt_box3ds`, they are the predictions and the targets on the validation set.\n",
    "Next steps: \n",
    "* Compute mAP on the validation set using the evaluation script provided in the SDK\n",
    "* Run inference on the test set.\n",
    "* Making a submission.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model limitations\n",
    "- The model performs very poorly on uncommon classes.\n",
    "- The boxes are imprecise: the input has a very low resolution (one pixel is 40x40cm in the real world!), and we arbitrarily threshold the predictions and fit boxes around these boxes. As we evaluate with IoUs between 0.4 and 0.75, we can expect that to hurt the score.\n",
    "- The model is barely converged - we could train for longer.\n",
    "- We only use LIDAR data, and we only use one lidar sweep.\n",
    "- We compress the height dimension into only 3 channels. We assume every object is 1.75 meters tall and is at the same height of the ego vehicle, which is surely a wrong assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree(train_data_folder)\n",
    "shutil.rmtree(validation_data_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
